Summary of AI Methods (Aligned with AITEL Reporting Checklist)

1. Protocols and Scoping

AI system: OpenAI gpt-5-mini (multimodal text + vision model).

Purpose: Assess surgical residents‚Äô verification-of-proficiency (VOP) suturing videos.

Input: Extracted video frames at 1 frame per second (fps).

Output: Narrative description of technique quality, mapped to an established 8-point faculty rubric (content validity previously established by the surgical education team).

Goal: Evaluate whether GPT-5-mini can produce structured assessments comparable to faculty scoring.

2. Modelling and Code

Platform: Python 3.11 using the OpenAI API (multimodal endpoints).

Process: Frames ‚Üí frame-by-frame interpretation ‚Üí temporal narrative synthesis ‚Üí rubric mapping.

Architecture: No custom model training; used OpenAI-hosted model. Code controlled random seed for reproducibility.

3. Algorithm Design

GPT-5-mini is a transformer-based large language + vision model (a scaled variant of GPT-5).

No fine-tuning performed; the model was prompted using structured, deterministic text instructions.

Prompts emphasized objective description (e.g., suture handling, needle angle, knot security) followed by rubric alignment.

4. Training Data (if applicable)

None locally applied. GPT-5-mini used its general pretrained parameters (non-domain-specific).

No surgical data or resident video content used for further training. All inference-only.

5. Testing and Validation

Videos scored independently by two faculty raters using the same 8-point rubric.

AI scores (from GPT-5-mini) compared with faculty consensus scores for agreement analysis (Spearman œÅ and mean absolute error).

6. Use and Validation of Known System

The AI model was tested in a realistic VOP environment (de-identified surgical skills lab recordings).

Frame extraction and model calls were automated to standardize exposure.

No real-time feedback or interactivity; evaluation was post-hoc.

7. Comparisons

Primary comparison: GPT-5-mini vs. human faculty rubric ratings.

Exploratory comparison: GPT-5-mini vs. ChatGPT 4-Vision (pilot) to assess stability across model families.

8. Real-World Requirements

Human-in-the-loop validation required for any summative use.

Current use limited to formative proof-of-concept; not deployed for credentialing decisions.

9. Results

Narrative outputs produced per video, structured along rubric dimensions.

Agreement with human raters moderate-to-high for technical domains (e.g., needle handling) and lower for subjective domains (e.g., economy of motion).

Demonstrated feasibility of AI-augmented assessment from low-frame-rate video.

10. Discussion / Limitations

Model hallucination risk minimized by prompt constraints, but subjectivity persists.

Performance depends on video clarity, lighting, and angle.

No generalization to live assessment yet; purely retrospective.

11. Ethics

All videos de-identified before analysis.

IRB exemption obtained as educational program evaluation.

No resident identifiers stored; outputs used only for aggregate research reporting.

üß© Additional Materials Requested

Model: GPT-5-mini (text + vision) via OpenAI API, 2025-Q2 version.

Prompts: Structured to (1) describe observed actions; (2) generate a procedural narrative; (3) map to each rubric item (1‚Äì8).
Example structure available on request (it includes instructional text ensuring no coaching or remediation).

Faculty Rubric: 8-item suturing proficiency rubric (needle handling, bite placement, knot security, tension, economy of motion, flow, tissue handling, overall efficiency).